import streamlit as st
import os
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import Ollama


# ========== Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø© ==========
st.set_page_config(page_title="ğŸ¤– Helper to learn Deep Learning", layout="wide")

# ========== Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ ==========
with st.sidebar:
    st.image("https://upload.wikimedia.org/wikipedia/commons/e/e3/Deep_learning.png", width=250)
    st.markdown("## Ø¹Ù† Ø§Ù„ØªØ·Ø¨ÙŠÙ‚")
    st.markdown("""
    This application is designed to help you learn about Deep Learning by answering your questions using a combination of a local LLM and a vector store for efficient retrieval of relevant information.    
    - âœ…Work by using `llama3.1` as the LLM
    - âœ… Using `HuggingFaceEmbeddings` for text embeddings
    - âœ… Using FAISS as a vector store for efficient retrieval
    """)
    st.markdown("### ğŸ‘¨â€ğŸ’»Developer:[@Mohamed Bakrey](https://github.com/mohamedbakrey12)")
    st.markdown("---")
    if st.button("ğŸ”„ Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"):
        st.session_state.chat_history = []
        st.experimental_rerun()

# ========== Ø§Ù„Ø¹Ù†ÙˆØ§Ù† ==========
st.markdown("<h1 style='text-align: center; color: #green;'>ğŸ’¡ Helper to learn Deep Learning</h1>", unsafe_allow_html=True)
st.markdown("<p style='text-align: center;'>Ask any question on Deep learning.</p>", unsafe_allow_html=True)
st.markdown("---")

# ========== ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ==========
@st.cache_resource
def load_llm():
    return Ollama(model="llama3.1", temperature=0.1)

@st.cache_resource
def load_retriever():
    embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
    vectorstore = FAISS.load_local("faiss_store", embedding, allow_dangerous_deserialization=True)
    return vectorstore.as_retriever()

llm = load_llm()
retriever = load_retriever()
qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)

# ========== ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ==========
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# ========== ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ ==========
with st.container():
    st.markdown("### ğŸ—¨ï¸ Ask now:")
    user_input = st.text_input("âœï¸ Write youe question here:", key="input")

    if user_input:
        answer = qa_chain.run(user_input)
        st.session_state.chat_history.append({"question": user_input, "answer": answer})

# ========== Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ==========
if st.session_state.chat_history:
    st.markdown("## ğŸ“œHistory of chats")
    for i, chat in enumerate(reversed(st.session_state.chat_history)):
        st.markdown(f"""
        <div style="background-color:#green;padding:15px;border-radius:10px;margin-bottom:10px;">
            <b>ğŸ§‘â€ğŸ“ Question? </b> {chat['question']}<br><br>
            <b>ğŸ¤– Result: </b> {chat['answer']}
        </div>
        """, unsafe_allow_html=True)
